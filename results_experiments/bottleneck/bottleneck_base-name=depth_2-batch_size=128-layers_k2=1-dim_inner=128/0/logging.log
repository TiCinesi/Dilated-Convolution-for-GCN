GraphGymModule(
  (model): DilatedPositionalGNNModel(
    (encoder): FeatureEncoder(
      (node_encoder): BottleneckEncoder(
        (embedding_list): ModuleList(
          (0): Embedding(5, 128)
          (1): Embedding(5, 128)
        )
      )
    )
    (mp): GNNDilatedPositionalStage(
      (classic_layers): ModuleList(
        (0): GeneralLayer(
          (layer): LayerGINConv(
            (model): GINConv(nn=Sequential(
              (0): Linear(128, 128, bias=True)
              (1): ReLU()
              (2): Linear(128, 128, bias=True)
            ))
          )
          (post_layer): Sequential(
            (0): ReLU()
          )
        )
      )
      (dilated_layers): ModuleList(
        (0): DilatedPositionalGeneralLayer(
          (layer): LayerGINConv(
            (model): GINConv(nn=Sequential(
              (0): Linear(128, 128, bias=True)
              (1): ReLU()
              (2): Linear(128, 128, bias=True)
            ))
          )
          (post_layer): Sequential(
            (0): ReLU()
          )
          (gem): GeM(p=3.0000, eps=1e-06)
          (pos): PositionalEncoding()
        )
      )
      (act): ReLU()
    )
    (post_mp): BottleneckHead(
      (layer_post_mp): Linear(in_features=128, out_features=5, bias=False)
    )
  )
)
accelerator: cuda
benchmark: False
bn:
  eps: 1e-05
  mom: 0.1
cfg_dest: config.yaml
custom_metrics: []
dataset:
  cache_load: False
  cache_save: False
  dir: ./datasets
  edge_dim: 0
  edge_encoder: False
  edge_encoder_bn: False
  edge_encoder_name: Bond
  edge_message_ratio: 0.8
  edge_negative_sampling_ratio: 1.0
  edge_train_mode: all
  encoder: True
  encoder_bn: True
  encoder_dim: 128
  encoder_name: db
  format: BOTTLENECK
  label_column: none
  label_table: none
  location: local
  name: depth_2
  node_encoder: True
  node_encoder_bn: False
  node_encoder_name: bottleneck_encoder
  positional_encoding_path: False
  preprocesss_dataset: True
  remove_feature: False
  resample_disjoint: False
  resample_negative: False
  shuffle_split: True
  split: [0.8, 0.1, 0.1]
  split_mode: random
  task: graph
  task_type: classification
  to_undirected: False
  transductive: False
  transform: none
  tu_simple: True
  use_sparse_adj: False
devices: None
gnn:
  act: relu
  act_on_last_layer_mp: True
  agg: none
  att_final_linear: False
  att_final_linear_bn: False
  att_heads: 4
  att_heads_final: 6
  batchnorm: False
  clear_feature: True
  dilated_path_join: add
  dim_inner: 128
  dropout: 0.0
  edge_agg: add
  head: bottleneck_head
  keep_edge: 0.5
  l2norm: False
  layer_norm: True
  layer_type: ginconv_paper
  layer_type_dilated: ginconv_paper
  layers_k1: 1
  layers_k2: 1
  layers_mp: 3
  layers_post_mp: 1
  layers_pre_mp: 0
  learn_alpha_residual_connection: True
  msg_direction: single
  normalize_adj: False
  self_msg: concat
  skip_every: 1
  stage_type: skipsum
  use_edge_features: False
gpu_mem: False
mem:
  inplace: False
metric_agg: argmax
metric_best: accuracy
model:
  edge_decoding: dot
  graph_pooling: mean
  loss_fun: cross_entropy
  match_upper: True
  size_average: mean
  thresh: 0.5
  type: dilapos_gnn
num_threads: 6
num_workers: 16
optim:
  base_lr: 0.001
  lr_decay: 0.1
  max_epoch: 100
  momentum: 0.9
  optimizer: adam
  scheduler: reduce_lr_on_plateau
  step_gamma: 0.5
  step_size: 10
  steps: [30, 60, 90]
  weight_decay: 0.0
out_dir: results_experiments/bottleneck_base_grid_depths_layers_small/bottleneck_base-name=depth_2-batch_size=128-layers_k2=1-dim_inner=128
persistent_workers: False
print: both
round: 4
run_dir: results_experiments/bottleneck_base_grid_depths_layers_small/bottleneck_base-name=depth_2-batch_size=128-layers_k2=1-dim_inner=128/0
seed: 1
share:
  dim0: 4
  dim_in: 2
  dim_out: 4
  num_splits: 2
tensorboard_agg: True
tensorboard_each_run: False
train:
  accumulate_grad: 1
  auto_resume: False
  batch_size: 128
  ckpt_clean: True
  ckpt_period: 100
  compute_test: False
  early_stopping: True
  early_stopping_patience: 50
  enable_ckpt: True
  epoch_resume: -1
  eval_period: 1
  iter_per_epoch: 32
  monitor_val: False
  neighbor_sizes: [20, 15, 10, 5]
  node_per_graph: 32
  radius: extend
  sample_node: False
  sampler: full_batch
  skip_train_eval: False
  walk_length: 4
train_strategy: None
val:
  node_per_graph: 32
  radius: extend
  sample_node: False
  sampler: full_batch
view_emb: False
Num parameters: 67970
val: {'epoch': 0, 'loss': 1.9138, 'lr': 0.001, 'params': 67970, 'time_iter': 0.3824, 'accuracy': 0.1}
train: {'epoch': 0, 'eta': 3293.3953, 'loss': 1.3784, 'lr': 0.001, 'params': 67970, 'time_iter': 0.5544, 'accuracy': 0.3871}
val: {'epoch': 1, 'loss': 1.7175, 'lr': 0.001, 'params': 67970, 'time_iter': 0.3888, 'accuracy': 0.5}
train: {'epoch': 1, 'eta': 3427.708, 'loss': 0.6041, 'lr': 0.001, 'params': 67970, 'time_iter': 0.6114, 'accuracy': 0.8192}
val: {'epoch': 2, 'loss': 1.8096, 'lr': 0.001, 'params': 67970, 'time_iter': 0.3829, 'accuracy': 0.65}
train: {'epoch': 2, 'eta': 3517.596, 'loss': 0.0134, 'lr': 0.001, 'params': 67970, 'time_iter': 0.6473, 'accuracy': 1.0}
val: {'epoch': 3, 'loss': 1.8875, 'lr': 0.001, 'params': 67970, 'time_iter': 0.4037, 'accuracy': 0.65}
train: {'epoch': 3, 'eta': 3506.1774, 'loss': 0.0021, 'lr': 0.001, 'params': 67970, 'time_iter': 0.6217, 'accuracy': 1.0}
val: {'epoch': 4, 'loss': 1.9345, 'lr': 0.001, 'params': 67970, 'time_iter': 0.4027, 'accuracy': 0.6}
train: {'epoch': 4, 'eta': 3497.5538, 'loss': 0.0011, 'lr': 0.001, 'params': 67970, 'time_iter': 0.6332, 'accuracy': 1.0}
val: {'epoch': 5, 'loss': 1.9571, 'lr': 0.001, 'params': 67970, 'time_iter': 0.4037, 'accuracy': 0.65}
train: {'epoch': 5, 'eta': 3509.3379, 'loss': 0.0007, 'lr': 0.001, 'params': 67970, 'time_iter': 0.6653, 'accuracy': 1.0}
val: {'epoch': 6, 'loss': 1.93, 'lr': 0.001, 'params': 67970, 'time_iter': 0.3836, 'accuracy': 0.65}
train: {'epoch': 6, 'eta': 3467.3265, 'loss': 0.0005, 'lr': 0.001, 'params': 67970, 'time_iter': 0.6164, 'accuracy': 1.0}
val: {'epoch': 7, 'loss': 1.9307, 'lr': 0.001, 'params': 67970, 'time_iter': 0.3917, 'accuracy': 0.65}
train: {'epoch': 7, 'eta': 3441.4053, 'loss': 0.0004, 'lr': 0.001, 'params': 67970, 'time_iter': 0.6379, 'accuracy': 1.0}
val: {'epoch': 8, 'loss': 1.9493, 'lr': 0.001, 'params': 67970, 'time_iter': 0.384, 'accuracy': 0.65}
train: {'epoch': 8, 'eta': 3408.7119, 'loss': 0.0003, 'lr': 0.001, 'params': 67970, 'time_iter': 0.6312, 'accuracy': 1.0}
val: {'epoch': 9, 'loss': 1.9275, 'lr': 0.001, 'params': 67970, 'time_iter': 0.3823, 'accuracy': 0.65}
train: {'epoch': 9, 'eta': 3372.5167, 'loss': 0.0002, 'lr': 0.001, 'params': 67970, 'time_iter': 0.6266, 'accuracy': 1.0}
val: {'epoch': 10, 'loss': 1.934, 'lr': 0.001, 'params': 67970, 'time_iter': 0.394, 'accuracy': 0.65}
train: {'epoch': 10, 'eta': 3338.5615, 'loss': 0.0002, 'lr': 0.001, 'params': 67970, 'time_iter': 0.6318, 'accuracy': 1.0}
val: {'epoch': 11, 'loss': 1.9377, 'lr': 0.001, 'params': 67970, 'time_iter': 0.4035, 'accuracy': 0.65}
train: {'epoch': 11, 'eta': 3295.4507, 'loss': 0.0001, 'lr': 0.001, 'params': 67970, 'time_iter': 0.6125, 'accuracy': 1.0}
val: {'epoch': 12, 'loss': 1.9262, 'lr': 0.001, 'params': 67970, 'time_iter': 0.3853, 'accuracy': 0.65}
train: {'epoch': 12, 'eta': 3260.8946, 'loss': 0.0001, 'lr': 0.001, 'params': 67970, 'time_iter': 0.6313, 'accuracy': 1.0}
val: {'epoch': 13, 'loss': 1.9167, 'lr': 0.001, 'params': 67970, 'time_iter': 0.3846, 'accuracy': 0.65}
train: {'epoch': 13, 'eta': 3229.5596, 'loss': 0.0001, 'lr': 0.001, 'params': 67970, 'time_iter': 0.6414, 'accuracy': 1.0}
val: {'epoch': 14, 'loss': 1.9158, 'lr': 0.0005, 'params': 67970, 'time_iter': 0.3921, 'accuracy': 0.65}
train: {'epoch': 14, 'eta': 3189.9841, 'loss': 0.0001, 'lr': 0.0005, 'params': 67970, 'time_iter': 0.6199, 'accuracy': 1.0}
val: {'epoch': 15, 'loss': 1.914, 'lr': 0.0005, 'params': 67970, 'time_iter': 0.3809, 'accuracy': 0.65}
train: {'epoch': 15, 'eta': 3161.4196, 'loss': 0.0001, 'lr': 0.0005, 'params': 67970, 'time_iter': 0.6539, 'accuracy': 1.0}
val: {'epoch': 16, 'loss': 1.9145, 'lr': 0.0005, 'params': 67970, 'time_iter': 0.3833, 'accuracy': 0.65}
train: {'epoch': 16, 'eta': 3123.7657, 'loss': 0.0001, 'lr': 0.0005, 'params': 67970, 'time_iter': 0.6272, 'accuracy': 1.0}
val: {'epoch': 17, 'loss': 1.9182, 'lr': 0.0005, 'params': 67970, 'time_iter': 0.3862, 'accuracy': 0.65}
train: {'epoch': 17, 'eta': 3080.986, 'loss': 0.0001, 'lr': 0.0005, 'params': 67970, 'time_iter': 0.6084, 'accuracy': 1.0}
val: {'epoch': 18, 'loss': 1.9186, 'lr': 0.0005, 'params': 67970, 'time_iter': 0.395, 'accuracy': 0.65}
train: {'epoch': 18, 'eta': 3044.5956, 'loss': 0.0001, 'lr': 0.0005, 'params': 67970, 'time_iter': 0.6308, 'accuracy': 1.0}
val: {'epoch': 19, 'loss': 1.921, 'lr': 0.0005, 'params': 67970, 'time_iter': 0.3871, 'accuracy': 0.65}
train: {'epoch': 19, 'eta': 3005.153, 'loss': 0.0001, 'lr': 0.0005, 'params': 67970, 'time_iter': 0.6187, 'accuracy': 1.0}
val: {'epoch': 20, 'loss': 1.9118, 'lr': 0.0005, 'params': 67970, 'time_iter': 0.3895, 'accuracy': 0.65}
train: {'epoch': 20, 'eta': 2966.6418, 'loss': 0.0001, 'lr': 0.0005, 'params': 67970, 'time_iter': 0.6219, 'accuracy': 1.0}
val: {'epoch': 21, 'loss': 1.917, 'lr': 0.0005, 'params': 67970, 'time_iter': 0.395, 'accuracy': 0.65}
train: {'epoch': 21, 'eta': 2926.8703, 'loss': 0.0001, 'lr': 0.0005, 'params': 67970, 'time_iter': 0.6154, 'accuracy': 1.0}
val: {'epoch': 22, 'loss': 1.9051, 'lr': 0.0005, 'params': 67970, 'time_iter': 0.3872, 'accuracy': 0.65}
train: {'epoch': 22, 'eta': 2890.925, 'loss': 0.0001, 'lr': 0.0005, 'params': 67970, 'time_iter': 0.6333, 'accuracy': 1.0}
val: {'epoch': 23, 'loss': 1.9153, 'lr': 0.0005, 'params': 67970, 'time_iter': 0.3865, 'accuracy': 0.65}
train: {'epoch': 23, 'eta': 2852.6043, 'loss': 0.0, 'lr': 0.0005, 'params': 67970, 'time_iter': 0.6217, 'accuracy': 1.0}
val: {'epoch': 24, 'loss': 1.8995, 'lr': 0.0005, 'params': 67970, 'time_iter': 0.4082, 'accuracy': 0.65}
train: {'epoch': 24, 'eta': 2814.5846, 'loss': 0.0, 'lr': 0.0005, 'params': 67970, 'time_iter': 0.6229, 'accuracy': 1.0}
val: {'epoch': 25, 'loss': 1.9085, 'lr': 0.0003, 'params': 67970, 'time_iter': 0.3903, 'accuracy': 0.65}
train: {'epoch': 25, 'eta': 2774.9239, 'loss': 0.0, 'lr': 0.0003, 'params': 67970, 'time_iter': 0.613, 'accuracy': 1.0}
val: {'epoch': 26, 'loss': 1.9096, 'lr': 0.0003, 'params': 67970, 'time_iter': 0.3886, 'accuracy': 0.65}
train: {'epoch': 26, 'eta': 2734.9576, 'loss': 0.0, 'lr': 0.0003, 'params': 67970, 'time_iter': 0.6098, 'accuracy': 1.0}
val: {'epoch': 27, 'loss': 1.9065, 'lr': 0.0003, 'params': 67970, 'time_iter': 0.391, 'accuracy': 0.65}
train: {'epoch': 27, 'eta': 2694.7503, 'loss': 0.0, 'lr': 0.0003, 'params': 67970, 'time_iter': 0.6066, 'accuracy': 1.0}
val: {'epoch': 28, 'loss': 1.911, 'lr': 0.0003, 'params': 67970, 'time_iter': 0.3888, 'accuracy': 0.65}
train: {'epoch': 28, 'eta': 2660.6552, 'loss': 0.0, 'lr': 0.0003, 'params': 67970, 'time_iter': 0.6465, 'accuracy': 1.0}
val: {'epoch': 29, 'loss': 1.9103, 'lr': 0.0003, 'params': 67970, 'time_iter': 0.3867, 'accuracy': 0.65}
train: {'epoch': 29, 'eta': 2622.3447, 'loss': 0.0, 'lr': 0.0003, 'params': 67970, 'time_iter': 0.6186, 'accuracy': 1.0}
val: {'epoch': 30, 'loss': 1.9083, 'lr': 0.0003, 'params': 67970, 'time_iter': 0.3847, 'accuracy': 0.65}
train: {'epoch': 30, 'eta': 2584.7223, 'loss': 0.0, 'lr': 0.0003, 'params': 67970, 'time_iter': 0.6232, 'accuracy': 1.0}
val: {'epoch': 31, 'loss': 1.9086, 'lr': 0.0003, 'params': 67970, 'time_iter': 0.3858, 'accuracy': 0.65}
train: {'epoch': 31, 'eta': 2551.0644, 'loss': 0.0, 'lr': 0.0003, 'params': 67970, 'time_iter': 0.6541, 'accuracy': 1.0}
val: {'epoch': 32, 'loss': 1.9062, 'lr': 0.0003, 'params': 67970, 'time_iter': 0.3879, 'accuracy': 0.65}
train: {'epoch': 32, 'eta': 2512.8057, 'loss': 0.0, 'lr': 0.0003, 'params': 67970, 'time_iter': 0.6192, 'accuracy': 1.0}
val: {'epoch': 33, 'loss': 1.9049, 'lr': 0.0003, 'params': 67970, 'time_iter': 0.3988, 'accuracy': 0.65}
train: {'epoch': 33, 'eta': 2474.4753, 'loss': 0.0, 'lr': 0.0003, 'params': 67970, 'time_iter': 0.618, 'accuracy': 1.0}
val: {'epoch': 34, 'loss': 1.9074, 'lr': 0.0003, 'params': 67970, 'time_iter': 0.3898, 'accuracy': 0.65}
train: {'epoch': 34, 'eta': 2439.8449, 'loss': 0.0, 'lr': 0.0003, 'params': 67970, 'time_iter': 0.6505, 'accuracy': 1.0}
val: {'epoch': 35, 'loss': 1.9058, 'lr': 0.0003, 'params': 67970, 'time_iter': 0.3922, 'accuracy': 0.65}
train: {'epoch': 35, 'eta': 2403.9082, 'loss': 0.0, 'lr': 0.0003, 'params': 67970, 'time_iter': 0.6406, 'accuracy': 1.0}
val: {'epoch': 36, 'loss': 1.9059, 'lr': 0.0001, 'params': 67970, 'time_iter': 0.3856, 'accuracy': 0.65}
train: {'epoch': 36, 'eta': 2367.1721, 'loss': 0.0, 'lr': 0.0001, 'params': 67970, 'time_iter': 0.6341, 'accuracy': 1.0}
val: {'epoch': 37, 'loss': 1.9039, 'lr': 0.0001, 'params': 67970, 'time_iter': 0.4063, 'accuracy': 0.65}
train: {'epoch': 37, 'eta': 2329.8711, 'loss': 0.0, 'lr': 0.0001, 'params': 67970, 'time_iter': 0.629, 'accuracy': 1.0}
val: {'epoch': 38, 'loss': 1.9046, 'lr': 0.0001, 'params': 67970, 'time_iter': 0.3886, 'accuracy': 0.65}
train: {'epoch': 38, 'eta': 2293.3271, 'loss': 0.0, 'lr': 0.0001, 'params': 67970, 'time_iter': 0.6373, 'accuracy': 1.0}
val: {'epoch': 39, 'loss': 1.907, 'lr': 0.0001, 'params': 67970, 'time_iter': 0.3946, 'accuracy': 0.65}
train: {'epoch': 39, 'eta': 2255.8559, 'loss': 0.0, 'lr': 0.0001, 'params': 67970, 'time_iter': 0.628, 'accuracy': 1.0}
val: {'epoch': 40, 'loss': 1.904, 'lr': 0.0001, 'params': 67970, 'time_iter': 0.4195, 'accuracy': 0.65}
train: {'epoch': 40, 'eta': 2221.2911, 'loss': 0.0, 'lr': 0.0001, 'params': 67970, 'time_iter': 0.6618, 'accuracy': 1.0}
val: {'epoch': 41, 'loss': 1.9055, 'lr': 0.0001, 'params': 67970, 'time_iter': 0.3892, 'accuracy': 0.65}
train: {'epoch': 41, 'eta': 2183.9506, 'loss': 0.0, 'lr': 0.0001, 'params': 67970, 'time_iter': 0.6312, 'accuracy': 1.0}
val: {'epoch': 42, 'loss': 1.9039, 'lr': 0.0001, 'params': 67970, 'time_iter': 0.3922, 'accuracy': 0.65}
train: {'epoch': 42, 'eta': 2147.4774, 'loss': 0.0, 'lr': 0.0001, 'params': 67970, 'time_iter': 0.6424, 'accuracy': 1.0}
val: {'epoch': 43, 'loss': 1.9042, 'lr': 0.0001, 'params': 67970, 'time_iter': 0.3896, 'accuracy': 0.65}
train: {'epoch': 43, 'eta': 2109.2043, 'loss': 0.0, 'lr': 0.0001, 'params': 67970, 'time_iter': 0.6201, 'accuracy': 1.0}
val: {'epoch': 44, 'loss': 1.9049, 'lr': 0.0001, 'params': 67970, 'time_iter': 0.3867, 'accuracy': 0.65}
train: {'epoch': 44, 'eta': 2097.7609, 'loss': 0.0, 'lr': 0.0001, 'params': 67970, 'time_iter': 0.9853, 'accuracy': 1.0}
val: {'epoch': 45, 'loss': 1.903, 'lr': 0.0001, 'params': 67970, 'time_iter': 0.3847, 'accuracy': 0.65}
train: {'epoch': 45, 'eta': 2059.7165, 'loss': 0.0, 'lr': 0.0001, 'params': 67970, 'time_iter': 0.6371, 'accuracy': 1.0}
val: {'epoch': 46, 'loss': 1.902, 'lr': 0.0001, 'params': 67970, 'time_iter': 0.3882, 'accuracy': 0.65}
train: {'epoch': 46, 'eta': 2019.1161, 'loss': 0.0, 'lr': 0.0001, 'params': 67970, 'time_iter': 0.5994, 'accuracy': 1.0}
val: {'epoch': 47, 'loss': 1.9032, 'lr': 0.0001, 'params': 67970, 'time_iter': 0.3894, 'accuracy': 0.65}
train: {'epoch': 47, 'eta': 1982.0556, 'loss': 0.0, 'lr': 0.0001, 'params': 67970, 'time_iter': 0.6509, 'accuracy': 1.0}
val: {'epoch': 48, 'loss': 1.9026, 'lr': 0.0001, 'params': 67970, 'time_iter': 0.3894, 'accuracy': 0.65}
train: {'epoch': 48, 'eta': 1944.4591, 'loss': 0.0, 'lr': 0.0001, 'params': 67970, 'time_iter': 0.6436, 'accuracy': 1.0}
val: {'epoch': 49, 'loss': 1.903, 'lr': 0.0001, 'params': 67970, 'time_iter': 0.3839, 'accuracy': 0.65}
train: {'epoch': 49, 'eta': 1905.3442, 'loss': 0.0, 'lr': 0.0001, 'params': 67970, 'time_iter': 0.619, 'accuracy': 1.0}
val: {'epoch': 50, 'loss': 1.9026, 'lr': 0.0001, 'params': 67970, 'time_iter': 0.3788, 'accuracy': 0.65}
train: {'epoch': 50, 'eta': 1867.4308, 'loss': 0.0, 'lr': 0.0001, 'params': 67970, 'time_iter': 0.6385, 'accuracy': 1.0}
val: {'epoch': 51, 'loss': 1.9021, 'lr': 0.0001, 'params': 67970, 'time_iter': 0.39, 'accuracy': 0.65}
train: {'epoch': 51, 'eta': 1829.3998, 'loss': 0.0, 'lr': 0.0001, 'params': 67970, 'time_iter': 0.6366, 'accuracy': 1.0}
val: {'epoch': 52, 'loss': 1.9036, 'lr': 0.0001, 'params': 67970, 'time_iter': 0.3849, 'accuracy': 0.65}
train: {'epoch': 52, 'eta': 1790.3188, 'loss': 0.0, 'lr': 0.0001, 'params': 67970, 'time_iter': 0.617, 'accuracy': 1.0}
Results aggregated across runs saved in results_experiments/bottleneck_base_grid_depths_layers_small/bottleneck_base-name=depth_2-batch_size=128-layers_k2=1-dim_inner=128/agg
